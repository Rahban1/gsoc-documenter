var documenterSearchIndex = {"docs":
[{"location":"improving_tokenizer.html#Updating-Tokenizer","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"","category":"section"},{"location":"improving_tokenizer.html#Understanding-the-task","page":"Updating Tokenizer","title":"Understanding the task","text":"","category":"section"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"A tokenizer is a component of a search engine that processes text by breaking it down into smaller units called tokens. These tokens are then indexed and used to match search queries against the content.","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"Right now the tokenize function in the repository in assets/html/js/search.js is :","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"    tokenize: (string) => string.split(/[\\s\\-\\.]+/)","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"It splits the string into tokens using regular expression that match spaces (\\s), hyphens (-), and dots (.) as delimiters. Consider somebody search for tasklocalstorage, a default tokenizer will split it into the following tokens “task”, “local”, “storage” While this works well for general text, it can cause problems when searching for programming related content, especially in languages like Julia where identifiers often contain underscores and are meant to be treated as a single cohesive unit.","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"Some problems that can arise from this approach is :","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"Loss of context\nReduced precision\nPoor user experience","category":"page"},{"location":"improving_tokenizer.html#The-problem","page":"Updating Tokenizer","title":"The problem","text":"","category":"section"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"The old tokenizer was too simple and caused these issues:","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"Problem 1: Lost Important Julia Syntax\nWhat happened when someone searched for \"Base.sort\": \"Base.sort function\" → [\"Base\", \"sort\", \"function\"]\nLost the connection between \"Base\" and \"sort\"\nWhy this is bad: Julia has special syntax like Base.sort where the dot (.) connects the module name to the function name. The old tokenizer split on dots, so it couldn't find \"Base.sort\" when you searched for it!\nProblem 2: Operators Got Lost\nWhen someone searched for \"^\" (power operator): \"Use ^ operator\" → [\"Use\", \"\", \"operator\"] The \"^\" symbol disappeared!\nProblem 3: Macros Broken Apart\nWhen someone searched for \"@time\" (a Julia macro): \"@time macro\" → [\"\", \"time\", \"macro\"] Lost the \"@\" symbol that makes it a macro!","category":"page"},{"location":"improving_tokenizer.html#Solution","page":"Updating Tokenizer","title":"Solution","text":"","category":"section"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"Change 1: Improving on the custom trimmer","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"right now the trimmer is :","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"  word = word\n    .replace(/^[^a-zA-Z0-9@!]+/, \"\")\n    .replace(/[^a-zA-Z0-9@!]+$/, \"\")","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"Basically, what this is doing is removing every character which is not uppercase letters, lowercase letters, numbers, and two symbols '@' and '!' from the beginning and end of the query","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"but the problem is what if somebody search for just '^' it would result nothing","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"Change 2: Intelligent Pattern Matching\nI completely rewrote the tokenizer to understand Julia's special syntax. Here's what I did:","category":"page"},{"location":"improving_tokenizer.html","page":"Updating Tokenizer","title":"Updating Tokenizer","text":"    // Julia-aware tokenization that preserves meaningful syntax elements\n    tokenize: (string) => {\n      const tokens = [];\n      let remaining = string;\n      \n      // Julia-specific patterns to preserve as complete tokens\n      const patterns = [\n        // Module qualified names (e.g., Base.sort, Module.Submodule.function)\n        /\\b[A-Z][A-Za-z0-9_]*(?:\\.[A-Z][A-Za-z0-9_]*)*\\.[a-z_][A-Za-z0-9_!]*\\b/g,\n        // Macro calls (e.g., @time, @async)\n        /@[A-Za-z_][A-Za-z0-9_]*/g,\n        // Type parameters (e.g., Array{T,N}, Vector{Int})\n        /\\b[A-Z][A-Za-z0-9_]*\\{[^}]+\\}/g,\n        // Function names with module qualification (e.g., Base.+, Base.:^)\n        /\\b[A-Z][A-Za-z0-9_]*\\.:[A-Za-z0-9_!+\\-*/^&|%<>=.]+/g,\n        // Operators as complete tokens (e.g., !=, &&, ||, ^, .=, ->)\n        /[!<>=+\\-*/^&|%:.]+/g,\n        // Function signatures with type annotations (e.g., f(x::Int))\n        /\\b[a-z_][A-Za-z0-9_!]*\\([^)]*::[^)]*\\)/g,\n        // Regular identifiers and function names\n        /\\b[A-Za-z_][A-Za-z0-9_!]*\\b/g,\n        // Numbers (integers, floats, scientific notation)\n        /\\b\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?\\b/g\n      ];\n      \n      // Apply patterns in order of specificity (most specific first)\n      for (const pattern of patterns) {\n        pattern.lastIndex = 0; // Reset regex state\n        let match;\n        while ((match = pattern.exec(remaining)) !== null) {\n          const token = match[0].trim();\n          if (token && !tokens.includes(token)) {\n            tokens.push(token);\n          }\n        }\n      }\n      \n      // Also split on common delimiters for any remaining content\n      const basicTokens = remaining.split(/[\\s\\-,;()[\\]{}]+/).filter(t => t.trim());\n      for (const token of basicTokens) {\n        if (token && !tokens.includes(token)) {\n          tokens.push(token);\n        }\n      }\n      \n      return tokens.filter(token => token.length > 0);\n    },","category":"page"},{"location":"adding_search_benchmarks.html#Adding-Search-Benchmarks","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"We had our first meeting, and we discussed what would be the flow of the entire internshiop and also discussed how to go about the first deliverable as per the proposal which is Adding Search Benchmarks.  ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"We discussed what should be the language of choice for writing scripts for benchmarking, we had two possible candidates, one was Julia (for obvious reasons, since the whole repo is in Julia) and the other one was JavaScript since the search functionality is implemented in JavaScript so it would be easier to interact with the search functionality.  ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"We talked about it and thought of JavaScript as a better choice but now I think of it, I belive the barebone architecture for benchmarks should be in Julia only so that in future if anybody want to add more benchmarks or new tests they can do it easily as I am expecting most of the people coming in the Documenter repo are coming from Julia background and as far as interacting to the JavaScript based search functionality we can see how to do it through Julia in coming days.","category":"page"},{"location":"adding_search_benchmarks.html#Creating-query-structure","page":"Adding Search Benchmarks","title":"Creating query structure","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"First we'll create a new directory in test folder, I have named it search. Inside it I have created the first file named test_queries.jl","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"The file structure look like this :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"test/\n├─search/\n│   └─test_queries.jl\n...","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"I started with creating a basic struct which stores the search query and what should be the expected docs in the following manner :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"struct TestQuery\n    query::String\n    expected_docs::Vector{String}\nend","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"we can then compare it with the actual result and find out the different benchmarks.","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Now we can create different groups of queries like basic queries or queries specific to Julia syntax and if anybody from the community want to test some queries specific to their usecase, they can do it easily. We can then use them all together using something like vcat which will concatenate all the arrays into one","category":"page"},{"location":"adding_search_benchmarks.html#Evaluation","page":"Adding Search Benchmarks","title":"Evaluation","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"For now, I am using three metrics for calculating benchmarks namely :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Precision \nmeasures how many of the returned results are relevant.\nExample: if you returned 5 docs, out of which 3 are relevant, precision = 3/5 = 0.6.\nRecall \nmeasures how many of the true relevant documents were found in the result.\nExample: if there were 4 relevant docs and you returned 3 of them, recall = 3/4 = 0.75.\nF1 Score \nharmonic mean of precision and recall.\nthis balances precision and recall in a single number.\nF_1 = 2 times fractextprecision times textrecalltextprecision + textrecall","category":"page"},{"location":"adding_search_benchmarks.html#Helper-functions","page":"Adding Search Benchmarks","title":"Helper functions","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Now let's create a function that evaluate all these metrics for a single query","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"It'll look something like this :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"function evaluate_query(search_function, query::TestQuery)\n    results = search_function(query.query)\n\n    precision = calculate_precision(results, query.expected_docs)\n    recall = calculate_recall(results, query.expected_docs)\n    f1 = calculate_f1(precision, recall)\n\n    return Dict(\n        \"query\" => query.query,\n        \"precision\" => precision,\n        \"recall\" => recall,\n        \"f1\" => f1,\n        \"expected\" => query.expected_docs,\n        \"actual\" => results\n    )\nend","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"This will return a dictionary that have all the relevant results. We still have to create the search function that will search the query in our actual search implementation.","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"This looks good, now we need to create a function that evaluate all metrics for a suite of queries, which would essentially be calling the evaluate_query function for array of queries, and then calculating the mean of all results for each metric and return a dictionary similar to evaluate_query function","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"It look something like this : ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"function evaluate_all(search_function, queries)\n    results = [evaluate_query(search_function, q) for q in queries]\n\n    avg_precision = mean([r[\"precision\"] for r in results])\n    avg_recall = mean([r[\"recall\"] for r in results])\n    avg_f1 = mean([r[\"f1\"] for r in results])\n\n    return Dict(\n        \"individual_results\" => results,\n        \"average_precision\" => avg_precision,\n        \"average_recall\" => avg_recall,\n        \"average_f1_score\" => avg_f1\n    )\nend","category":"page"},{"location":"adding_search_benchmarks.html#The-Meeting-#2","page":"Adding Search Benchmarks","title":"The Meeting #2","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"We had our weekly meeting and there were few suggested edits which we are going to implement :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"use struct instead of dictionary to return the search results.\njust display the overall result in the terminal and rest all of the detailed results should be written in a text file.\nthe returning struct should also contain integers like total_documents_retrieved, total_relevant_found along with float. \nWrite short, descriptive comments explain the code\nmy mentors has advised me to open a pr, so that other people can see and give their suggestions on the work done till now how here the open pr link : PR Link","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"We have now created this struct for a single search query ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"struct QueryResult\n    query::String\n    precision::Float64\n    recall::Float64\n    f1::Float64\n    expected::Vector{String}\n    actual::Vector{String}\n    # Raw integer values used in calculations\n    relevant_count::Int  # Number of relevant documents found\n    total_retrieved::Int  # Total number of documents retrieved\n    total_relevant::Int   # Total number of relevant documents\nend","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"and one for multiple search queries","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"struct EvaluationResults\n    individual_results::Vector{QueryResult}\n    average_precision::Float64\n    average_recall::Float64\n    average_f1_score::Float64\n    # Raw integer values for overall evaluation\n    total_relevant_found::Int    # Total number of relevant documents found across all queries\n    total_documents_retrieved::Int  # Total number of documents retrieved across all queries\n    total_relevant_documents::Int   # Total number of relevant documents across all queries\nend","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"also I have done relevant changes to the previously made functions and now from each function we are returning values with the struct only, much more robust!","category":"page"},{"location":"adding_search_benchmarks.html#Printing-the-Benchmarks","page":"Adding Search Benchmarks","title":"Printing the Benchmarks","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"we are writing the overall results in the cli and the detailed results are written in a file which would be named : searchbenchmarkresultsyyyy-mm-ddHH-MM-SS.txt, where the placeholders will be filled by the date and time when that file was build.","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"right now we are just displaying it as it is but my maintainer has suggested to use a Julia package named PrettyTables.jl","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"right now this is how the results in CLI are looking ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"(Image: CLI Output)","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"we can definitely make it prettier using PrettyTable.jl","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"and now after using the PrettyTable.jl package, it is looking like this : ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"(Image: CLI Output new)","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Looking much better! (IMO Obviously)","category":"page"},{"location":"adding_search_benchmarks.html#Imitating-the-Search","page":"Adding Search Benchmarks","title":"Imitating the Search","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Now since we want to search for the query and compare it with the expected result we want to imitate the search functionality originally implemented in the Documenter which uses the minisearch engine","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"so the steps would look like :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Loading the search index\nInstall the minisearch using npm\nWriting the js code to use the minisearch engine\nRun the code\nReturn the results as actual docs for comparison with expected docs","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Let's start with loading the search index. Now we have thought of multiple ways of going about this, the main challenge was where should we get the search index from, our options are :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"using the search index built during the test process :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"search_index_path = joinpath(@__DIR__, \"../examples/builds/html/search_index.js\")","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"using the actual search index built during the build process of the Documenter documentation","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"    search_index_path = joinpath(@__DIR__, \"../../docs/build/search_index.js\")","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"I have gone with the later, since if we use the search index that is used in production we can do more thorugh testing","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"so the whole function now look like this : ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"    # Load the real search index from test examples (already built!)\n    function load_real_search_index()\n        # Use the example search index that's already built and tested\n        search_index_path = joinpath(@__DIR__, \"../../docs/build/search_index.js\")\n\n        if !isfile(search_index_path)\n            error(\"Search index not found at: $search_index_path\")\n        end\n\n        # Read and parse the JavaScript file\n        content = read(search_index_path, String)\n\n        # Find the JSON data after \"var documenterSearchIndex = \"\n        json_start = findfirst(\"var documenterSearchIndex = \", content)\n        if json_start === nothing\n            error(\"Invalid search index format: missing variable declaration\")\n        end\n\n        # Extract JSON content (everything after the variable declaration)\n        json_content = content[(last(json_start) + 1):end]\n\n        # Parse the JSON\n        parsed = JSON.parse(json_content)\n        return parsed[\"docs\"]  # Return just the docs array\n    end","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"if the file doesn't exist we throw an error, else we read the file using the read function available in Julia, now the search index file has structure like this : ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"var documenterSearchIndex = {\"docs\":[{\"location\":\"linenumbers/#@repl,-@example,-and-@eval-have-correct-LineNumberNodes-inserted\",\"page\":\"@repl, @example, and @eval have correct LineNumberNodes inserted\"...}]","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"so we access the search index by removing the var documenterSearchIndex = part by storing its last index and we store everything after that in json_content array, then we parse it using JSON.parse and from it return the value of parsed[\"docs\"] to finally get the complete search index in JSON format.","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Now we'll install the minisearch using npm for this we just did created a package.json and added minisearch as a dependency, here a little hiccup came since I initially used ^6.1.0 for the version but my mentors advised to use the exact version which is getting used in the Documenter which is 6.1.0 so I fixed it, here's what the package.json looks like:","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"{\n\"name\": \"documenter-search-benchmark\",\n\"version\": \"1.0.0\",\n\"description\": \"Search benchmarking for Documenter.jl\",\n\"dependencies\": {\n    \"minisearch\": \"^6.1.0\"\n}\n}","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"next step, we'll be writing the JS code to use the minisearch engine Now my initial thought would be that this is a piece of cake, just call the search functionality already implemented in assets/html/js/search.js and we are good to go but what I didn't realize is that they both have different execution environments, the original search functionality is designed for the browser, where ours is a Julia script, it runs in a command line environment using Node.js as a subprocess to execute JavaScript. so now we have two options :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Replicate the core logic\nPros : no new dependence\nCons : Violates DRY\nIsolate the pure search logic in another file and then call it in both the places assets/html/js/search.js and test/search/real_search.jl\nPros : Obeys the DRY principle\nCons : have to add a new build tool to a primarily Julia project","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"I did gave a try to the second option but finally I have gone with the first approach as it is more simple and since it is primarily a Julia project I don't want to add unnecessary JS dependencies in it.","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"so initially I created a string only with all the JS code and named it wrapper_js and just read it, but that end up becoming a very big string so my mentor suggested to have a seperate .js file and read it from there so we are doing that and injecting data using placeholders so now we don't have to spin the full browser to test the search functionality, which would be much slower","category":"page"},{"location":"adding_search_benchmarks.html#Adding-Make-Command","page":"Adding Search Benchmarks","title":"Adding Make Command","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"My mentor suggested to add a make command that run these tests, I named it search-benchmarks","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"the command is pretty simple : ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"search-benchmarks:\n\t${JULIA} --project test/search/run_benchmarks.jl","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"added ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"rm -f test/search/search_benchmark_results_*.txt","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"to the clean command","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"and added the search-benchmarks command to the PHONY so it doesn't interpret it as a file rather than a command","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":".PHONY: default docs-instantiate themes help changelog docs test search-benchmarks","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"and finally added this","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"@echo \" - make search-benchmarks: run search functionality benchmarks\"","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"in the help command to tell the people what does the command do","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"and now our command works like a charm!","category":"page"},{"location":"adding_search_benchmarks.html#Running-Benchmarks-on-CI","page":"Adding Search Benchmarks","title":"Running Benchmarks on CI","text":"","category":"section"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"My mentor asked to add a simple CI job that run these benchmarks, so I created a Github Actions workflow, named it Benchmarks (quite creative!) ","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"so whenever code is pushed to the master repo, or when someone opens a pull request or when it is triggered manually and ensuring only one workflow run at a time using concurrency for the same branch or pull request it does :","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"Checkouts the code \nSetup Julia\nCache the Julia packages\nBuild Julia packages\nRun Benchmarks","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"After I showed it to my mentor he suggested to also upload the full benchmark output as an artifact which can be downloaded by anybody, great idea!","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"but later he suggested a much better idea, earlier I had a seperate CI job named benchmark.yml so he suggested to not make a seperate job rather put this in the old CI.yml and make it depend on the main suite where the manual is build anyway so we could upload that from the CI run and then download that into this benchmarking job, so we don't have to rebuild the manual for benchmark again","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"so this is what the final job looks like inside CI.yml","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"benchmarks:\n    name: Julia ${{ matrix.version }} - ${{ matrix.os }} - ${{ matrix.arch }}\n    runs-on: ${{ matrix.os }}\n    needs: docs\n    strategy:\n      fail-fast: false\n      matrix:\n        version:\n          - '1'\n        os:\n          - ubuntu-latest\n        arch:\n          - x64\n    steps:\n      - uses: actions/checkout@v4\n      - uses: julia-actions/setup-julia@v2\n        with:\n          version: ${{ matrix.version }}\n          arch: ${{ matrix.arch }}\n          show-versioninfo: true\n      - uses: julia-actions/cache@v2\n      - uses: julia-actions/julia-buildpkg@v1\n      - name: Download search index\n        uses: actions/download-artifact@v4\n        with:\n          name: search-index\n          path: docs/build\n      - name: Build test examples\n        shell: julia --color=yes --project=test/examples {0}\n        run: |\n          using Pkg\n          Pkg.instantiate()\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20.x'\n      - name: Install Node.js dependencies\n        run: npm install\n        working-directory: test/search\n      - name: Run search benchmarks\n        run: make search-benchmarks\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Upload search benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: search-benchmark-results\n          path: test/search/search_benchmark_results_*.txt","category":"page"},{"location":"adding_search_benchmarks.html","page":"Adding Search Benchmarks","title":"Adding Search Benchmarks","text":"so now in CI the test are running taking the search index from the previous CI and then giving the ability to download the detailed benchmark report. Neat!","category":"page"},{"location":"index.html#Improving-Search-functionality-for-Documenter.jl","page":"Home","title":"Improving Search functionality for Documenter.jl","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"This site is a collection of notes, progress reports for the 2025 Google Summer of Code (GSoC) project by @Rahban1, mentored by @mortenpi and @Hetarth02.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"I thought it would be a good idea to document my journey using Documenter itself, also I was inspired by my mentor Morten who did the same :)","category":"page"}]
}
