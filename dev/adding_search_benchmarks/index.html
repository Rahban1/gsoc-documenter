<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Adding Search Benchmarks · GSoC Documentation</title><meta name="title" content="Adding Search Benchmarks · GSoC Documentation"/><meta property="og:title" content="Adding Search Benchmarks · GSoC Documentation"/><meta property="twitter:title" content="Adding Search Benchmarks · GSoC Documentation"/><meta name="description" content="Documentation for GSoC Documentation."/><meta property="og:description" content="Documentation for GSoC Documentation."/><meta property="twitter:description" content="Documentation for GSoC Documentation."/><meta property="og:url" content="https://rahban1.github.io/gsoc-documenter/adding_search_benchmarks/"/><meta property="twitter:url" content="https://rahban1.github.io/gsoc-documenter/adding_search_benchmarks/"/><link rel="canonical" href="https://rahban1.github.io/gsoc-documenter/adding_search_benchmarks/"/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX-X"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-XXXXXXXXX-X', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="GSoC Documentation logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GSoC Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Deliverable Summary</span><ul><li class="is-active"><a class="tocitem" href>Adding Search Benchmarks</a><ul class="internal"><li><a class="tocitem" href="#Creating-query-structure"><span>Creating query structure</span></a></li><li><a class="tocitem" href="#Evaluation"><span>Evaluation</span></a></li><li><a class="tocitem" href="#Helper-functions"><span>Helper functions</span></a></li><li><a class="tocitem" href="#The-Meeting-#2"><span>The Meeting #2</span></a></li><li><a class="tocitem" href="#Printing-the-Benchmarks"><span>Printing the Benchmarks</span></a></li><li><a class="tocitem" href="#Imitating-the-Search"><span>Imitating the Search</span></a></li><li><a class="tocitem" href="#Adding-Make-Command"><span>Adding Make Command</span></a></li><li><a class="tocitem" href="#Running-Benchmarks-on-CI"><span>Running Benchmarks on CI</span></a></li></ul></li><li><a class="tocitem" href="../creating_test_manual/">Creating test manual and testing it on benchmarks</a></li><li><a class="tocitem" href="../improving_tokenizer/">Updating Tokenizer</a></li><li><a class="tocitem" href="../warn_when_si_big/">Warn when the search index is too big</a></li><li><a class="tocitem" href="../listing_in_si/">Remove the page category from the search index and make everything a section</a></li><li><a class="tocitem" href="../dev_docs_for_search/">Documenter.jl Search System Developer Documentation</a></li></ul></li><li><span class="tocitem">Additionals</span><ul><li><a class="tocitem" href="../key_bindings/">Navigate the Search Results using up and down keys</a></li></ul></li><li><a class="tocitem" href="../conclusion/">Conclusion</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Deliverable Summary</a></li><li class="is-active"><a href>Adding Search Benchmarks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Adding Search Benchmarks</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Rahban1/gsoc-documenter" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/Rahban1/gsoc-documenter/blob/main/docs/src/adding_search_benchmarks.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Adding-Search-Benchmarks"><a class="docs-heading-anchor" href="#Adding-Search-Benchmarks">Adding Search Benchmarks</a><a id="Adding-Search-Benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-Search-Benchmarks" title="Permalink"></a></h1><h3 id="Relevant-PR-:-[#2740](https://github.com/JuliaDocs/Documenter.jl/pull/2740)"><a class="docs-heading-anchor" href="#Relevant-PR-:-[#2740](https://github.com/JuliaDocs/Documenter.jl/pull/2740)">Relevant PR : <a href="https://github.com/JuliaDocs/Documenter.jl/pull/2740">#2740</a></a><a id="Relevant-PR-:-[#2740](https://github.com/JuliaDocs/Documenter.jl/pull/2740)-1"></a><a class="docs-heading-anchor-permalink" href="#Relevant-PR-:-[#2740](https://github.com/JuliaDocs/Documenter.jl/pull/2740)" title="Permalink"></a></h3><p>We had our first meeting, and we discussed what would be the flow of the entire internshiop and also discussed how to go about the first deliverable as per the proposal which is <strong>Adding Search Benchmarks</strong>.  </p><p>We discussed what should be the language of choice for writing scripts for benchmarking, we had two possible candidates, one was Julia (for obvious reasons, since the whole repo is in Julia) and the other one was JavaScript since the search functionality is implemented in JavaScript so it would be easier to interact with the search functionality.  </p><p>We talked about it and thought of JavaScript as a better choice but now I think of it, I belive the barebone architecture for benchmarks should be in Julia only so that in future if anybody want to add more benchmarks or new tests they can do it easily as I am expecting most of the people coming in the Documenter repo are coming from Julia background and as far as interacting to the JavaScript based search functionality we can see how to do it through Julia in coming days.</p><h2 id="Creating-query-structure"><a class="docs-heading-anchor" href="#Creating-query-structure">Creating query structure</a><a id="Creating-query-structure-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-query-structure" title="Permalink"></a></h2><p>First we&#39;ll create a new directory in test folder, I have named it <strong>search</strong>. Inside it I have created the first file named <strong>test_queries.jl</strong></p><p>The file structure look like this :</p><pre><code class="nohighlight hljs">test/
├─search/
│   └─test_queries.jl
...</code></pre><p>I started with creating a basic struct which stores the search query and what should be the expected docs in the following manner :</p><pre><code class="nohighlight hljs">struct TestQuery
    query::String
    expected_docs::Vector{String}
end</code></pre><p>we can then compare it with the actual result and find out the different benchmarks.</p><p>Now we can create different groups of queries like basic queries or queries specific to Julia syntax and if anybody from the community want to test some queries specific to their usecase, they can do it easily. We can then use them all together using something like vcat which will concatenate all the arrays into one</p><h2 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h2><p>For now, I am using three metrics for calculating benchmarks namely :</p><ul><li>Precision <ul><li>measures how many of the returned results are relevant.</li><li><em>Example</em>: if you returned 5 docs, out of which 3 are relevant, precision = 3/5 = 0.6.</li></ul></li><li>Recall <ul><li>measures how many of the true relevant documents were found in the result.</li><li><em>Example</em>: if there were 4 relevant docs and you returned 3 of them, recall = 3/4 = 0.75.</li></ul></li><li>F1 Score <ul><li>harmonic mean of precision and recall.</li><li>this balances precision and recall in a single number.</li><li><p class="math-container">\[F_1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}\]</p></li></ul></li></ul><h2 id="Helper-functions"><a class="docs-heading-anchor" href="#Helper-functions">Helper functions</a><a id="Helper-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Helper-functions" title="Permalink"></a></h2><p>Now let&#39;s create a function that evaluate all these metrics for a single query</p><p>It&#39;ll look something like this :</p><pre><code class="language-julia hljs">function evaluate_query(search_function, query::TestQuery)
    results = search_function(query.query)

    precision = calculate_precision(results, query.expected_docs)
    recall = calculate_recall(results, query.expected_docs)
    f1 = calculate_f1(precision, recall)

    return Dict(
        &quot;query&quot; =&gt; query.query,
        &quot;precision&quot; =&gt; precision,
        &quot;recall&quot; =&gt; recall,
        &quot;f1&quot; =&gt; f1,
        &quot;expected&quot; =&gt; query.expected_docs,
        &quot;actual&quot; =&gt; results
    )
end</code></pre><p>This will return a dictionary that have all the relevant results. We still have to create the search function that will search the query in our actual search implementation.</p><p>This looks good, now we need to create a function that evaluate all metrics for a suite of queries, which would essentially be calling the <code>evaluate_query</code> function for array of queries, and then calculating the mean of all results for each metric and return a dictionary similar to <code>evaluate_query</code> function</p><p>It look something like this : </p><pre><code class="language-julia hljs">function evaluate_all(search_function, queries)
    results = [evaluate_query(search_function, q) for q in queries]

    avg_precision = mean([r[&quot;precision&quot;] for r in results])
    avg_recall = mean([r[&quot;recall&quot;] for r in results])
    avg_f1 = mean([r[&quot;f1&quot;] for r in results])

    return Dict(
        &quot;individual_results&quot; =&gt; results,
        &quot;average_precision&quot; =&gt; avg_precision,
        &quot;average_recall&quot; =&gt; avg_recall,
        &quot;average_f1_score&quot; =&gt; avg_f1
    )
end</code></pre><h2 id="The-Meeting-#2"><a class="docs-heading-anchor" href="#The-Meeting-#2">The Meeting #2</a><a id="The-Meeting-#2-1"></a><a class="docs-heading-anchor-permalink" href="#The-Meeting-#2" title="Permalink"></a></h2><p>We had our weekly meeting and there were few suggested edits which we are going to implement :</p><ul><li>use struct instead of dictionary to return the search results.</li><li>just display the overall result in the terminal and rest all of the detailed results should be written in a text file.</li><li>the returning struct should also contain integers like <code>total_documents_retrieved, total_relevant_found</code> along with float. </li><li>Write short, descriptive comments explain the code</li><li>my mentors has advised me to open a pr, so that other people can see and give their suggestions on the work done till now how here the open pr link : <a href="https://github.com/JuliaDocs/Documenter.jl/pull/2740">PR Link</a></li></ul><p>We have now created this struct for a single search query </p><pre><code class="language-julia hljs">struct QueryResult
    query::String
    precision::Float64
    recall::Float64
    f1::Float64
    expected::Vector{String}
    actual::Vector{String}
    # Raw integer values used in calculations
    relevant_count::Int  # Number of relevant documents found
    total_retrieved::Int  # Total number of documents retrieved
    total_relevant::Int   # Total number of relevant documents
end</code></pre><p>and one for multiple search queries</p><pre><code class="language-julia hljs">struct EvaluationResults
    individual_results::Vector{QueryResult}
    average_precision::Float64
    average_recall::Float64
    average_f1_score::Float64
    # Raw integer values for overall evaluation
    total_relevant_found::Int    # Total number of relevant documents found across all queries
    total_documents_retrieved::Int  # Total number of documents retrieved across all queries
    total_relevant_documents::Int   # Total number of relevant documents across all queries
end</code></pre><p>also I have done relevant changes to the previously made functions and now from each function we are returning values with the struct only, much more robust!</p><h2 id="Printing-the-Benchmarks"><a class="docs-heading-anchor" href="#Printing-the-Benchmarks">Printing the Benchmarks</a><a id="Printing-the-Benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#Printing-the-Benchmarks" title="Permalink"></a></h2><p>we are writing the overall results in the cli and the detailed results are written in a file which would be named : search<em>benchmark</em>results<em>yyyy-mm-dd</em>HH-MM-SS.txt, where the placeholders will be filled by the date and time when that file was build.</p><p>right now we are just displaying it as it is but my maintainer has suggested to use a Julia package named PrettyTables.jl</p><p>right now this is how the results in CLI are looking </p><p><img src="../assets/cli-output.png" alt="CLI Output"/></p><p>we can definitely make it prettier using <a href="https://ronisbr.github.io/PrettyTables.jl/stable/">PrettyTable.jl</a></p><p>and now after using the PrettyTable.jl package, it is looking like this : </p><p><img src="../assets/cli-output-new.png" alt="CLI Output new"/></p><p>Looking much better! (IMO Obviously)</p><h2 id="Imitating-the-Search"><a class="docs-heading-anchor" href="#Imitating-the-Search">Imitating the Search</a><a id="Imitating-the-Search-1"></a><a class="docs-heading-anchor-permalink" href="#Imitating-the-Search" title="Permalink"></a></h2><p>Now since we want to search for the query and compare it with the expected result we want to imitate the search functionality originally implemented in the Documenter which uses the minisearch engine</p><p>so the steps would look like :</p><ul><li>Loading the search index</li><li>Install the minisearch using npm</li><li>Writing the js code to use the minisearch engine</li><li>Run the code</li><li>Return the results as actual docs for comparison with expected docs</li></ul><p>Let&#39;s start with loading the search index. Now we have thought of multiple ways of going about this, the main challenge was where should we get the search index from, our options are :</p><ul><li>using the search index built during the test process :</li></ul><pre><code class="language-julia hljs">search_index_path = joinpath(@__DIR__, &quot;../examples/builds/html/search_index.js&quot;)</code></pre><ul><li>using the actual search index built during the build process of the Documenter documentation</li></ul><pre><code class="language-julia hljs">    search_index_path = joinpath(@__DIR__, &quot;../../docs/build/search_index.js&quot;)</code></pre><p>I have gone with the later, since if we use the search index that is used in production we can do more thorugh testing</p><p>so the whole function now look like this : </p><pre><code class="language-julia hljs">    # Load the real search index from test examples (already built!)
    function load_real_search_index()
        # Use the example search index that&#39;s already built and tested
        search_index_path = joinpath(@__DIR__, &quot;../../docs/build/search_index.js&quot;)

        if !isfile(search_index_path)
            error(&quot;Search index not found at: $search_index_path&quot;)
        end

        # Read and parse the JavaScript file
        content = read(search_index_path, String)

        # Find the JSON data after &quot;var documenterSearchIndex = &quot;
        json_start = findfirst(&quot;var documenterSearchIndex = &quot;, content)
        if json_start === nothing
            error(&quot;Invalid search index format: missing variable declaration&quot;)
        end

        # Extract JSON content (everything after the variable declaration)
        json_content = content[(last(json_start) + 1):end]

        # Parse the JSON
        parsed = JSON.parse(json_content)
        return parsed[&quot;docs&quot;]  # Return just the docs array
    end</code></pre><p>if the file doesn&#39;t exist we throw an error, else we read the file using the read function available in Julia, now the search index file has structure like this : </p><pre><code class="language-julia hljs">var documenterSearchIndex = {&quot;docs&quot;:[{&quot;location&quot;:&quot;linenumbers/#@repl,-@example,-and-@eval-have-correct-LineNumberNodes-inserted&quot;,&quot;page&quot;:&quot;@repl, @example, and @eval have correct LineNumberNodes inserted&quot;...}]</code></pre><p>so we access the search index by removing the <code>var documenterSearchIndex =</code> part by storing its last index and we store everything after that in <code>json_content</code> array, then we parse it using <code>JSON.parse</code> and from it return the value of parsed[&quot;docs&quot;] to finally get the complete search index in JSON format.</p><p>Now we&#39;ll install the minisearch using npm for this we just did created a package.json and added minisearch as a dependency, here a little hiccup came since I initially used <code>^6.1.0</code> for the version but my mentors advised to use the exact version which is getting used in the Documenter which is <code>6.1.0</code> so I fixed it, here&#39;s what the package.json looks like:</p><pre><code class="nohighlight hljs">{
&quot;name&quot;: &quot;documenter-search-benchmark&quot;,
&quot;version&quot;: &quot;1.0.0&quot;,
&quot;description&quot;: &quot;Search benchmarking for Documenter.jl&quot;,
&quot;dependencies&quot;: {
    &quot;minisearch&quot;: &quot;^6.1.0&quot;
}
}</code></pre><p>next step, we&#39;ll be writing the JS code to use the minisearch engine Now my initial thought would be that this is a piece of cake, just call the search functionality already implemented in <code>assets/html/js/search.js</code> and we are good to go but what I didn&#39;t realize is that they both have different execution environments, the original search functionality is designed for the browser, where ours is a Julia script, it runs in a command line environment using Node.js as a subprocess to execute JavaScript. so now we have two options :</p><ul><li>Replicate the core logic<ul><li>Pros : no new dependence</li><li>Cons : Violates DRY</li></ul></li><li>Isolate the pure search logic in another file and then call it in both the places <code>assets/html/js/search.js</code> and <code>test/search/real_search.jl</code><ul><li>Pros : Obeys the DRY principle</li><li>Cons : have to add a new build tool to a primarily Julia project</li></ul></li></ul><p>I did gave a try to the second option but finally I have gone with the first approach as it is more simple and since it is primarily a Julia project I don&#39;t want to add unnecessary JS dependencies in it.</p><p>so initially I created a string only with all the JS code and named it wrapper_js and just read it, but that end up becoming a very big string so my mentor suggested to have a seperate .js file and read it from there so we are doing that and injecting data using placeholders so now we don&#39;t have to spin the full browser to test the search functionality, which would be much slower</p><h2 id="Adding-Make-Command"><a class="docs-heading-anchor" href="#Adding-Make-Command">Adding Make Command</a><a id="Adding-Make-Command-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-Make-Command" title="Permalink"></a></h2><p>My mentor suggested to add a make command that run these tests, I named it <code>search-benchmarks</code></p><p>the command is pretty simple : </p><pre><code class="language-makefile hljs">search-benchmarks:
	${JULIA} --project test/search/run_benchmarks.jl</code></pre><p>added </p><pre><code class="language-makefile hljs">rm -f test/search/search_benchmark_results_*.txt</code></pre><p>to the clean command</p><p>and added the search-benchmarks command to the PHONY so it doesn&#39;t interpret it as a file rather than a command</p><pre><code class="language-makefile hljs">.PHONY: default docs-instantiate themes help changelog docs test search-benchmarks</code></pre><p>and finally added this</p><pre><code class="language-makefile hljs">@echo &quot; - make search-benchmarks: run search functionality benchmarks&quot;</code></pre><p>in the help command to tell the people what does the command do</p><p>and now our command works like a charm!</p><h2 id="Running-Benchmarks-on-CI"><a class="docs-heading-anchor" href="#Running-Benchmarks-on-CI">Running Benchmarks on CI</a><a id="Running-Benchmarks-on-CI-1"></a><a class="docs-heading-anchor-permalink" href="#Running-Benchmarks-on-CI" title="Permalink"></a></h2><p>My mentor asked to add a simple CI job that run these benchmarks, so I created a Github Actions workflow, named it <code>Benchmarks</code> (quite creative!) </p><p>so whenever code is pushed to the master repo, or when someone opens a pull request or when it is triggered manually and ensuring only one workflow run at a time using concurrency for the same branch or pull request it does :</p><ul><li>Checkouts the code </li><li>Setup Julia</li><li>Cache the Julia packages</li><li>Build Julia packages</li><li>Run Benchmarks</li></ul><p>After I showed it to my mentor he suggested to also upload the full benchmark output as an artifact which can be downloaded by anybody, great idea!</p><p>but later he suggested a much better idea, earlier I had a seperate CI job named benchmark.yml so he suggested to not make a seperate job rather put this in the old CI.yml and make it depend on the main suite where the manual is build anyway so we could upload that from the CI run and then download that into this benchmarking job, so we don&#39;t have to rebuild the manual for benchmark again</p><p>so this is what the final job looks like inside <code>CI.yml</code></p><pre><code class="language-yml hljs">benchmarks:
    name: Julia ${{ matrix.version }} - ${{ matrix.os }} - ${{ matrix.arch }}
    runs-on: ${{ matrix.os }}
    needs: docs
    strategy:
      fail-fast: false
      matrix:
        version:
          - &#39;1&#39;
        os:
          - ubuntu-latest
        arch:
          - x64
    steps:
      - uses: actions/checkout@v4
      - uses: julia-actions/setup-julia@v2
        with:
          version: ${{ matrix.version }}
          arch: ${{ matrix.arch }}
          show-versioninfo: true
      - uses: julia-actions/cache@v2
      - uses: julia-actions/julia-buildpkg@v1
      - name: Download search index
        uses: actions/download-artifact@v4
        with:
          name: search-index
          path: docs/build
      - name: Build test examples
        shell: julia --color=yes --project=test/examples {0}
        run: |
          using Pkg
          Pkg.instantiate()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - uses: actions/setup-node@v4
        with:
          node-version: &#39;20.x&#39;
      - name: Install Node.js dependencies
        run: npm install
        working-directory: test/search
      - name: Run search benchmarks
        run: make search-benchmarks
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Upload search benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: search-benchmark-results
          path: test/search/search_benchmark_results_*.txt</code></pre><p>so now in CI the test are running taking the search index from the previous CI and then giving the ability to download the detailed benchmark report. Neat!</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../creating_test_manual/">Creating test manual and testing it on benchmarks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Thursday 28 August 2025 19:21">Thursday 28 August 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
